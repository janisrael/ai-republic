#!/usr/bin/env python3
"""
LoRA Training Script for working-lora
Generated by AI Refinement Dashboard
"""

import os
import json
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    try:
        logger.info("üöÄ Starting LoRA training for working-lora")
        
        # Model and data paths
        base_model = "Qwen/Qwen2.5-7B"
        train_data_path = "training_data/job_22/train.jsonl"
        val_data_path = "training_data/job_22/val.jsonl"
        output_dir = f"models/working-lora_lora"
        
        # Load model with 8-bit quantization (memory efficient)
        logger.info("üì• Loading base model...")
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_8bit_use_double_quant=True,
            bnb_8bit_quant_type="nf8",
            bnb_8bit_compute_dtype=torch.bfloat16
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            base_model,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
        
        # Add padding token if missing
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Prepare model for k-bit training
        model = prepare_model_for_kbit_training(model)
        
        # LoRA configuration
        lora_config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # Apply LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # Load dataset
        logger.info("üìä Loading training data...")
        dataset = load_dataset(
            "json",
            data_files={
                "train": train_data_path,
                "validation": val_data_path
            },
            streaming=False
        )
        
        # Tokenize function
        def tokenize_function(examples):
            # Create instruction format
            texts = []
            for i in range(len(examples["instruction"])):
                instruction = examples["instruction"][i]
                input_text = examples["input"][i] if examples["input"][i] else ""
                output = examples["output"][i]
                
                if input_text:
                    text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
                else:
                    text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
                
                texts.append(text)
            
            # Tokenize
            tokenized = tokenizer(
                texts,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
            
            # Set labels same as input_ids
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        
        # Tokenize datasets
        train_dataset = dataset["train"].map(tokenize_function, batched=True)
        val_dataset = dataset["validation"].map(tokenize_function, batched=True)
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=4,
            num_train_epochs=1,
            learning_rate=0.0002,
            fp16=True,
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=50,
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            report_to=None,  # Disable wandb/tensorboard
            remove_unused_columns=False,
        )
        
        # Create trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
        )
        
        # Start training with progress tracking
        logger.info("üèÉ Starting training...")
        
        # Custom training loop with progress updates
        total_steps = trainer.get_train_dataloader().__len__() * 1
        current_step = 0
        
        for epoch in range(1):
            logger.info(f"üìö Starting epoch {epoch + 1}/1")
            epoch_steps = 0
            
            for step, batch in enumerate(trainer.get_train_dataloader()):
                # Training step
                trainer.training_step(trainer.model, batch)
                current_step += 1
                epoch_steps += 1
                
                # Update progress every 10 steps
                if current_step % 10 == 0:
                    progress = 0.2 + (current_step / total_steps) * 0.6  # 20% to 80%
                    logger.info(f"üìä Progress: {progress*100:.1f}% (Step {current_step}/{total_steps})")
                    
                    # Update database progress
                    import requests
                    try:
                        requests.post(f'http://localhost:5000/api/training-jobs/22/progress', 
                                    json={'progress': progress}, timeout=1)
                    except:
                        pass  # Don't fail training if progress update fails
            
            logger.info(f"‚úÖ Completed epoch {epoch + 1}/1")
        
        # Save model
        logger.info("üíæ Saving trained model...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        logger.info("‚úÖ LoRA training completed successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Training failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()
