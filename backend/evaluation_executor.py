#!/usr/bin/env python3
"""
Real Evaluation Executor for AI Refinement Dashboard
Implements actual model evaluation against datasets
"""

import os
import json
import time
import subprocess
import requests
from datetime import datetime
from typing import Dict, Any, List, Tuple
from database import db

class EvaluationExecutor:
    def __init__(self):
        self.running_evaluations = {}
    
    def start_evaluation(self, eval_id: int, eval_data: Dict[str, Any]) -> bool:
        """Start real evaluation for a model against a dataset"""
        try:
            # Update evaluation status to RUNNING
            db.update_evaluation(eval_id, {
                'status': 'RUNNING',
                'started_at': datetime.now().isoformat()
            })
            
            # Start evaluation in a separate thread
            import threading
            eval_thread = threading.Thread(
                target=self._execute_evaluation,
                args=(eval_id, eval_data)
            )
            eval_thread.daemon = True
            eval_thread.start()
            
            self.running_evaluations[eval_id] = {
                'thread': eval_thread,
                'status': 'RUNNING',
                'started_at': datetime.now()
            }
            
            return True
            
        except Exception as e:
            print(f"Error starting evaluation {eval_id}: {e}")
            db.update_evaluation(eval_id, {
                'status': 'FAILED',
                'error_message': str(e)
            })
            return False
    
    def _execute_evaluation(self, eval_id: int, eval_data: Dict[str, Any]):
        """Execute the actual evaluation process"""
        try:
            model_name = eval_data.get('model_name')
            dataset_id = eval_data.get('dataset_id')
            evaluation_type = eval_data.get('evaluation_type', 'accuracy')
            
            print(f"ðŸ§ª Starting evaluation: {model_name} vs dataset {dataset_id}")
            print(f"ðŸ“ Evaluation ID: {eval_id}, Type: {evaluation_type}")
            
            # Log to file for debugging
            self._log_evaluation(f"EVAL {eval_id}: Starting evaluation of {model_name} against dataset {dataset_id}")
            
            # Get dataset samples
            dataset = self._get_dataset(dataset_id)
            if not dataset:
                error_msg = f"Dataset {dataset_id} not found"
                self._log_evaluation(f"EVAL {eval_id}: ERROR - {error_msg}")
                raise Exception(error_msg)
            
            samples = self._get_dataset_samples(dataset)
            if not samples:
                error_msg = f"No samples found in dataset {dataset_id}"
                self._log_evaluation(f"EVAL {eval_id}: ERROR - {error_msg}")
                raise Exception(error_msg)
            
            print(f"ðŸ“Š Testing {model_name} against {len(samples)} samples")
            self._log_evaluation(f"EVAL {eval_id}: Testing {model_name} against {len(samples)} samples from dataset {dataset_id}")
            
            # Run evaluation with before/after metrics
            if evaluation_type == 'accuracy':
                results = self._evaluate_accuracy_with_baseline(model_name, samples, eval_data)
            elif evaluation_type == 'code_generation':
                results = self._evaluate_code_generation_with_baseline(model_name, samples, eval_data)
            else:
                results = self._evaluate_accuracy_with_baseline(model_name, samples, eval_data)  # Default to accuracy
            
            # Update evaluation with results
            db.update_evaluation(eval_id, {
                'status': 'COMPLETED',
                'completed_at': datetime.now().isoformat(),
                'before_metrics': results.get('before_metrics', {}),
                'after_metrics': results.get('after_metrics', {}),
                'improvement': results.get('improvement', 0),
                'notes': results.get('notes', 'Evaluation completed successfully')
            })
            
            print(f"âœ… Evaluation completed for {model_name}")
            
        except Exception as e:
            error_msg = f"âŒ Evaluation failed for {eval_id}: {e}"
            print(error_msg)
            self._log_evaluation(f"EVAL {eval_id}: ERROR - {str(e)}")
            import traceback
            traceback.print_exc()
            db.update_evaluation(eval_id, {
                'status': 'FAILED',
                'error_message': str(e),
                'completed_at': datetime.now().isoformat()
            })
    
    def _get_dataset(self, dataset_id: int) -> Dict[str, Any]:
        """Get dataset by ID"""
        all_datasets = db.get_all_datasets()
        for dataset in all_datasets:
            if dataset['id'] == dataset_id:
                return dataset
        return None
    
    def _get_dataset_samples(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract samples from dataset for evaluation"""
        metadata = dataset.get('metadata', {})
        # Use all_samples if available, otherwise fall back to samples_preview
        samples = metadata.get('all_samples', metadata.get('samples_preview', []))
        
        # Limit to first 3 samples for evaluation (to avoid timeout with large models)
        return samples[:3]
    
    def _evaluate_accuracy_with_baseline(self, model_name: str, samples: List[Dict[str, Any]], eval_data: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate model accuracy with before/after comparison"""
        print(f"ðŸŽ¯ Evaluating accuracy with baseline for {model_name}")
        self._log_evaluation(f"EVAL: Starting before/after evaluation for {model_name}")
        
        # Get base model from training job
        base_model = self._get_base_model_from_evaluation(eval_data)
        if not base_model:
            print("âš ï¸ No base model found, using default baseline")
            base_model = "llama3.2:latest"  # Default baseline
        
        print(f"ðŸ“Š Comparing {model_name} (after) vs {base_model} (before)")
        self._log_evaluation(f"EVAL: Comparing {model_name} vs {base_model}")
        
        # Test base model (before)
        before_metrics = self._test_model_performance(base_model, samples, "BEFORE")
        
        # Test fine-tuned model (after)  
        after_metrics = self._test_model_performance(model_name, samples, "AFTER")
        
        # Calculate improvement
        improvement = self._calculate_improvement(before_metrics, after_metrics)
        
        return {
            'before_metrics': before_metrics,
            'after_metrics': after_metrics,
            'improvement': improvement,
            'notes': f'Before: {base_model} ({before_metrics["accuracy"]:.1%}), After: {model_name} ({after_metrics["accuracy"]:.1%}), Improvement: {improvement:.1%}'
        }
    
    def _evaluate_accuracy(self, model_name: str, samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Evaluate model accuracy against samples"""
        print(f"ðŸŽ¯ Evaluating accuracy for {model_name}")
        
        total_samples = len(samples)
        correct_predictions = 0
        total_inference_time = 0
        predictions = []
        
        for i, sample in enumerate(samples):
            try:
                # Prepare test prompt
                test_prompt = self._prepare_test_prompt(sample)
                
                # Test model
                start_time = time.time()
                response = self._query_model(model_name, test_prompt)
                inference_time = time.time() - start_time
                
                total_inference_time += inference_time
                
                # Evaluate response (simple keyword matching for now)
                is_correct = self._evaluate_response(sample, response)
                if is_correct:
                    correct_predictions += 1
                
                predictions.append({
                    'sample_id': i,
                    'prompt': test_prompt,
                    'expected': sample.get('output', ''),
                    'actual': response,
                    'correct': is_correct,
                    'inference_time': inference_time
                })
                
                # Progress update every 10 samples
                if (i + 1) % 10 == 0:
                    progress = (i + 1) / total_samples
                    print(f"ðŸ“Š Progress: {progress*100:.1f}% ({i+1}/{total_samples})")
                
            except Exception as e:
                print(f"âš ï¸ Error testing sample {i}: {e}")
                continue
        
        # Calculate metrics
        accuracy = (correct_predictions / total_samples) * 100 if total_samples > 0 else 0
        avg_inference_time = total_inference_time / total_samples if total_samples > 0 else 0
        
        # Simulate before/after metrics (in real scenario, you'd compare against baseline)
        before_accuracy = max(0, accuracy - (10 + (accuracy * 0.1)))  # Simulate 10-20% improvement
        before_inference_time = avg_inference_time * 1.3  # Simulate 30% faster inference
        
        improvement = accuracy - before_accuracy
        
        return {
            'before_metrics': {
                'accuracy': round(before_accuracy, 1),
                'precision': round(before_accuracy / 100, 3),
                'recall': round(before_accuracy / 100, 3),
                'f1': round(before_accuracy / 100, 3),
                'inferenceTime': round(before_inference_time)
            },
            'after_metrics': {
                'accuracy': round(accuracy, 1),
                'precision': round(accuracy / 100, 3),
                'recall': round(accuracy / 100, 3),
                'f1': round(accuracy / 100, 3),
                'inferenceTime': round(avg_inference_time)
            },
            'improvement': round(improvement, 1),
            'notes': f'Evaluated {total_samples} samples. Model achieved {accuracy:.1f}% accuracy with {avg_inference_time:.1f}ms average inference time.',
            'predictions': predictions
        }
    
    def _prepare_test_prompt(self, sample: Dict[str, Any]) -> str:
        """Prepare test prompt from sample"""
        instruction = sample.get('instruction', '')
        input_text = sample.get('input', '')
        
        if input_text:
            return f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:"
        else:
            return f"### Instruction:\n{instruction}\n\n### Response:"
    
    def _query_model(self, model_name: str, prompt: str) -> str:
        """Query model via Ollama API"""
        try:
            # Use Ollama API to query the model
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': model_name,
                    'prompt': prompt,
                    'stream': False
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                raise Exception(f"Ollama API error: {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"Failed to query model {model_name}: {e}")
    
    def _evaluate_response(self, sample: Dict[str, Any], response: str) -> bool:
        """Evaluate if response is correct (simple keyword matching)"""
        expected = sample.get('output', '').lower()
        actual = response.lower()
        
        # Simple evaluation: check if key terms from expected output are in actual response
        if not expected:
            return False
        
        # Split expected into key terms
        expected_terms = [term.strip() for term in expected.split() if len(term.strip()) > 2]
        
        # Check if at least 50% of key terms are present in response
        matches = sum(1 for term in expected_terms if term in actual)
        match_ratio = matches / len(expected_terms) if expected_terms else 0
        
        return match_ratio >= 0.5
    
    def _evaluate_code_generation(self, model_name: str, samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Evaluate model for code generation tasks"""
        # Similar to accuracy evaluation but with code-specific metrics
        return self._evaluate_accuracy(model_name, samples)
    
    def stop_evaluation(self, eval_id: int) -> bool:
        """Stop a running evaluation"""
        try:
            if eval_id in self.running_evaluations:
                db.update_evaluation(eval_id, {
                    'status': 'STOPPED',
                    'completed_at': datetime.now().isoformat()
                })
                del self.running_evaluations[eval_id]
                print(f"ðŸ›‘ Stopped evaluation {eval_id}")
                return True
            else:
                print(f"âš ï¸ Evaluation {eval_id} is not running")
                return False
        except Exception as e:
            print(f"Error stopping evaluation {eval_id}: {e}")
            return False
    
    def _get_base_model_from_evaluation(self, eval_data: Dict[str, Any]) -> str:
        """Get base model from evaluation data"""
        # Try to get base model from training job
        try:
            # This would need to be passed from the evaluation creation
            return eval_data.get('base_model', '')
        except:
            return ''
    
    def _test_model_performance(self, model_name: str, samples: List[Dict[str, Any]], phase: str) -> Dict[str, Any]:
        """Test a single model's performance"""
        print(f"ðŸ§ª Testing {model_name} ({phase})")
        self._log_evaluation(f"EVAL: Testing {model_name} ({phase})")
        
        total_samples = len(samples)
        correct_predictions = 0
        total_inference_time = 0
        predictions = []
        
        for i, sample in enumerate(samples):
            try:
                # Prepare test prompt
                test_prompt = self._prepare_test_prompt(sample)
                
                # Test model
                start_time = time.time()
                response = self._query_model(model_name, test_prompt)
                inference_time = time.time() - start_time
                
                total_inference_time += inference_time
                
                # Simple accuracy check (you can make this more sophisticated)
                is_correct = self._check_prediction_accuracy(sample, response)
                if is_correct:
                    correct_predictions += 1
                
                predictions.append({
                    'sample_id': sample.get('id', i),
                    'prompt': test_prompt[:100] + '...',
                    'response': response[:100] + '...',
                    'correct': is_correct,
                    'inference_time': inference_time
                })
                
                print(f"  Sample {i+1}/{total_samples}: {'âœ…' if is_correct else 'âŒ'} ({inference_time:.2f}s)")
                
            except Exception as e:
                print(f"  Sample {i+1}/{total_samples}: âŒ Error - {e}")
                self._log_evaluation(f"EVAL: Error testing sample {i+1}: {e}")
                continue
        
        # Calculate metrics
        accuracy = correct_predictions / total_samples if total_samples > 0 else 0
        avg_inference_time = total_inference_time / total_samples if total_samples > 0 else 0
        
        return {
            'accuracy': accuracy,
            'precision': accuracy,  # Simplified for now
            'recall': accuracy,     # Simplified for now
            'f1': accuracy,         # Simplified for now
            'inferenceTime': avg_inference_time,
            'total_samples': total_samples,
            'correct_predictions': correct_predictions
        }
    
    def _calculate_improvement(self, before_metrics: Dict[str, Any], after_metrics: Dict[str, Any]) -> float:
        """Calculate improvement percentage"""
        before_acc = before_metrics.get('accuracy', 0)
        after_acc = after_metrics.get('accuracy', 0)
        
        if before_acc == 0:
            return after_acc * 100  # If baseline is 0, improvement is just the after score
        
        improvement = ((after_acc - before_acc) / before_acc) * 100
        return improvement
    
    def _check_prediction_accuracy(self, sample: Dict[str, Any], response: str) -> bool:
        """Check if the model's response is accurate (simplified)"""
        # This is a simplified accuracy check
        # In a real implementation, you'd have more sophisticated evaluation
        expected_output = sample.get('output', '').lower()
        response_lower = response.lower()
        
        # Simple keyword matching (you can make this more sophisticated)
        if expected_output and response_lower:
            # Check if key concepts from expected output appear in response
            expected_words = set(expected_output.split())
            response_words = set(response_lower.split())
            overlap = len(expected_words.intersection(response_words))
            return overlap >= len(expected_words) * 0.3  # 30% overlap threshold
        
        return False
    
    def _log_evaluation(self, message: str):
        """Log evaluation messages to file"""
        try:
            log_file = 'backend/evaluation.log'
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            with open(log_file, 'a') as f:
                f.write(f"[{timestamp}] {message}\n")
        except Exception as e:
            print(f"Failed to log evaluation message: {e}")

# Global evaluation executor instance
evaluation_executor = EvaluationExecutor()
